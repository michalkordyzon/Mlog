---
title: "Exploring Tiny Time-Mixer Model"
date: 2025-11-28T16:42:36+02:00
draft: false
tags: []
categories: []
ShowToc: false
cover:
  image: ""   # change to your image path in /static/images/
  alt: "Cover image"
  caption: "Optional caption for the cover image."
  relative: false
math: true
---

## Introduction
2024 was a year where time series zero/few shot forecasting functioality made its way into LLM architectures: Moment, TimesFM, Chonos, Moirai, and many more showed up. Those were novel models but, computationally, heavy to use. So soon the second wave of lighter models appeared incluging Tiny Time-Mixer (TTM) architecture from IBM Research. TTM can work with just 1M parameters, it supports channel correlations and exogenous signals, and handles multivariate forecasting. Let's dive in. 

TTM research paper [here](https://arxiv.org/pdf/2401.0395).


## Architecture

TTM model is based on TSMixer architecture (it came originally from MLPMixer, Google, 2021). In TSMixer multi-head attention is replaced with simpler mechanism: mixing information across time and features using simple MLPs (linear layers). This solves the problem of classic self-attention being to slow for long sequences. Self-attention compares each token to every other token, this is  O(nÂ²) complexity, so 'classic' attention layers scale quadratically with sequence length n. And the time series can be a very long sequences with thousands of tokens.

When it come to pre-training or finetuning TSMixer has few nice additions to original MLPMixer: 1) adaptive patching across layers, 2)diverse resolution sampling, 3)resolution prefix tuning, 4) multi-level modeling strategy. I will focus on last one, it's about modeling channel correlations on many levels at once: independent singular channel patterns, cross channel patterns, global patterns.
It's like modeling the data from following channels ton 3 levels simulatneusly:
- temperature
- traffic
- energy consumption
- humidity

One thing is to model each channe separately. TTMs can modeling these on three levels simultaneously.

1B samples used in model creation comes from Monash and Libcity datasets. Monash is ~250M samples and Libcity is the rest. Monash collects time series from many areas: weather, traffic, banking, bitcoin, eletricity, and more. Libcity is about city traffic time series. IBM's TTM models are open-sourced on huggingface, you can find more details in Training Data section here: https://huggingface.co/ibm-granite/granite-timeseries-ttm-r2





<figure>
  <img src="/images/0005_ttm_architecture.jpg" alt="Sample" width="700">
  <figcaption style="color: gray; font-style: normal; text-align: left;">
    TTM architecture
  </figcaption>
</figure>






<!-- <img src="/images/0001.jpg" alt="Sample" width="800"> -->




<!-- ## Datasets

Several interesting datasets were used for Granite Vision training. First is DocFM, a dataset created in IBM and mentioned in previous note on smalDocling model. Granite Vision Team focused on using only  a part of DocFM, specifically on synthetic visual QA datasets as a seed for creation synthetic dataset named DocFM verbalization-based VQA (DocFM VQA). Verbalized representation is simply descrption of visual elemnts like charts or pictures, etc. 
Similarly, next sythetic datasets were created: 
- Synthetic Chart VQA, synthetic chart with QA pairs
- Synthetic Flowchart VQA, synthetic flowcharts with QA pairs
- DocFM-ChartExtraction, synthetic dataset for teaching the Granive Vision how to extract data from tables
- DocFM Visual Cue and Captioning, 
- DocFM Rule Based Grounding  

Full corpus of datasets used for Granite Vision training is in the picture below.
<figure>
  <img src="/images/0004.jpg" alt="Sample" width="800">
  <figcaption style="color: gray; font-style: normal; text-align: left;">
    Granite Vision architecture
  </figcaption>
</figure>



## Data preprocessing

Data pre-processing consisted of several annotation and filtering procedures aimed at ensuring compliance with regulatory and safety standards as well as improving data quality.
Restricted Data removal: All public datasets were subject to a legal review to detect any potential licensing or PII-related risks. Data with unclear usage rights or originating outside the US was excluded.
CSAM removal: State-of-the-art NSFW detection systems were applied to eliminate all Child Sexual Abuse Material.
Face blurring: All visual PII was obfuscated by applying automated face-blurring techniques.
Deduplication: To avoid redundancy across merged collections, duplicate records were removed. Detection relied on exact pixel matching and perceptual hash methods to account for minor variations, as well as identification of duplicate text entries. -->

work in progress....

<!-- ## Training procecure -->

<!-- ## Evaluation -->