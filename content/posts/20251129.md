---
title: "Exploring Tiny Time Mixers Model"
date: 2025-11-28T16:42:36+02:00
draft: false
tags: []
categories: []
ShowToc: false
cover:
  image: ""   # change to your image path in /static/images/
  alt: "Cover image"
  caption: "Optional caption for the cover image."
  relative: false
math: true
---

## Introduction
2024 was a year where time series zero/few shot forecasting functioality made its way into LLM architectures: Moment, TimesFM, Chonos, Moirai, and many more showed up. Those were novel but, computationally, heavy to use. This triggered a second wave of lighter models incluging Tiny Time Mixers (TTM) architecture from IBM Research. TTM can work with just 1M parameters, supports  channel correlations and exogenous signals, and handles multivariate forecasting. Let's dive in. 

TTM research paper [here](https://arxiv.org/pdf/2401.03955).


## Architecture

TTM model is based on TSMixer architecture (which is based on MLPMixer, Google, 2021), where multi-head attention is replaced with 


<figure>
  <img src="/images/0005_ttm_architecture.jpg" alt="Sample" width="400">
  <figcaption style="color: gray; font-style: normal; text-align: left;">
    TTM architecture
  </figcaption>
</figure>






<!-- <img src="/images/0001.jpg" alt="Sample" width="800"> -->




<!-- ## Datasets

Several interesting datasets were used for Granite Vision training. First is DocFM, a dataset created in IBM and mentioned in previous note on smalDocling model. Granite Vision Team focused on using only  a part of DocFM, specifically on synthetic visual QA datasets as a seed for creation synthetic dataset named DocFM verbalization-based VQA (DocFM VQA). Verbalized representation is simply descrption of visual elemnts like charts or pictures, etc. 
Similarly, next sythetic datasets were created: 
- Synthetic Chart VQA, synthetic chart with QA pairs
- Synthetic Flowchart VQA, synthetic flowcharts with QA pairs
- DocFM-ChartExtraction, synthetic dataset for teaching the Granive Vision how to extract data from tables
- DocFM Visual Cue and Captioning, 
- DocFM Rule Based Grounding  

Full corpus of datasets used for Granite Vision training is in the picture below.
<figure>
  <img src="/images/0004.jpg" alt="Sample" width="800">
  <figcaption style="color: gray; font-style: normal; text-align: left;">
    Granite Vision architecture
  </figcaption>
</figure>



## Data preprocessing

Data pre-processing consisted of several annotation and filtering procedures aimed at ensuring compliance with regulatory and safety standards as well as improving data quality.
Restricted Data removal: All public datasets were subject to a legal review to detect any potential licensing or PII-related risks. Data with unclear usage rights or originating outside the US was excluded.
CSAM removal: State-of-the-art NSFW detection systems were applied to eliminate all Child Sexual Abuse Material.
Face blurring: All visual PII was obfuscated by applying automated face-blurring techniques.
Deduplication: To avoid redundancy across merged collections, duplicate records were removed. Detection relied on exact pixel matching and perceptual hash methods to account for minor variations, as well as identification of duplicate text entries. -->

work in progress....

<!-- ## Training procecure -->

<!-- ## Evaluation -->