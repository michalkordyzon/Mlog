---
title: "Granite Vision model notes"
date: 2025-08-26T16:42:36+02:00
draft: false
tags: []
categories: []
ShowToc: false
cover:
  image: ""   # change to your image path in /static/images/
  alt: "Cover image"
  caption: "Optional caption for the cover image."
  relative: false
math: true
---

## Introduction
Many VLMs perform great on benchmarks like viusal question-answering or multi-image reasoning. Models in 2024 were predominantly trained on natural images, thus often limiting performance in other domains like visual document undestanding.

Granite Vision is a 3 billion parameters model focused on entreprise use cases with visual document understanding document content extraction and working with documents. Get the Granite Vision research paper [here](https://arxiv.org/abs/2502.09927).


## Architecture

Granite Vision was trained on 12 trillion tokens. Dataset for it's training is constanlty curated by Granite Team but not open sourced. It contained 13 million images and 80 milllion instructions such as: document question-answering, scene understanding key-value extraction, text grounding, layout parsing, captioning, UI understanding, code comprehension. 



<figure>
  <img src="/images/0003.jpg" alt="Sample" width="400">
  <figcaption style="color: gray; font-style: normal; text-align: left;">
    Granite Vision architecture
  </figcaption>
</figure>

Granite Vision model has two modalities, so it has two encoders: text encoder, and vision encoder with projector that 'translates' image encodings to text. Image encoder uses SigLIP architecture (an advanced CLIP-like vision transformer). The input resolution is 384 × 384 pixels per patch. Important technique used here is concatenating features from multiple layers instead of just using last layer. This allows the model to 'see' details, high level features, and different color scales of the input image. Another jig is AnyRes that solves the problem of hard to read small letters, dense table, etc., in 384 X 384 resolution. AnyRes samples input image at multiple resoultions, by splitting the image on smaller patches, and fed througth visual encoder. This allows the model to learn and generalize across scales. 
Concatenating and AnyRes are especially useful for document understanding, which the main use case for Granite Vision.




<!-- <img src="/images/0001.jpg" alt="Sample" width="800"> -->




## Datasets

Several interesting datasets were used for Granite Vision training. First is DocFM, a dataset created in IBM and mentioned in previous note on smalDocling model. Granite Vision Team used only a part of DocFM, specifically: 

<!-- We can get bigger ```SmolDocling``` model (), or any large vision-language model to quickly get higher accuracy but also 'heavier' inference and so much bigger usage of compute. 
```SmolDocling``` can find right niche for deployments on edge devices or on any resource-constrained setting. Another usage is quick prototyping and  experimentation, it's always better to start with small and quick models and avoid complexity that comes with a size.   -->


<!-- Another interesting thing is a standard proposed by ```smolDocling``` model - ```DocTags```. It is created to use efficiently in inference and to train VLMs in a standardized way. HTML and Mardown formats are ambigous and by do not keep document layout context. ```DocTags``` separates text content from layout of document which bring clarity. ```DocTags``` has also clear and concise format which saves tokens and thus, inference and training on VLMs. See the basic example: 


HTML:
```
<h1>Invoice</h1><p>Customer Name: John Doe</p>
```
~20–25 tokens.



DocTags:
```
<heading>Invoice</heading><para>Customer Name: John Doe</para>
```
~12–15 tokens.
 -->



## Pre-training datasets
<!-- Seeing lack of good multimodal document data SmolDocling team created new public data set: ```DocLayNet-PT```. It contains 1.4M pages from ```DocFM``` dataset (PDF documents from CommonCrawl, Wikipedia, business domains).
Original ```SmolVLM``` had ```DocVQA``` capabilities (Document Visual Question Answering). To keep this feature the ```smolDocling``` was trained on Docmatix dataset with added ```DocTags``` format information.  -->



## Task-specific datasets
<!-- The model was also fine-tuned for specific tasks like recognition of layout, tables, charts, code, and equations.
For layout and tables the team prepared:
- 76k pages of human annotated and reviewed documets from ```DocLayNet-PT``` (created dataset was named ```DocLayNet v2```)
- 63k pages of tables and text from WordScape dataset
- 250k pages of synthetic annotations from wikpedia for layout, colors and fonts (created dataset was named ```SynthDocNet```)
Tables recognition were covered by fine-tuning with ```PubTables-1M```, ```FinTabNet```, ```WikiTableSet```, and tabular info from ```WordScape```. Table strcuture information was pushed into OTSL format, so that each cell tag had it's corresponding structure and text. -->

<!-- <figure>
  <img src="/images/0002.jpg" alt="Sample" width="800">
  <figcaption style="color: gray; font-style: normal; text-align: left;">
    Training datasets used for smolDocling
  </figcaption>
</figure> -->

## Experiments
<!-- To enhance recognition of specific elements and to introduce ability to write no-code instructions to smolDocling model the team has put rule-based techniques and ```Granite-3.1-2b-instruct``` model. Random elements were taken from ```DocLayNet-PT``` and according instructions for this element were created, something like: "Perform OCR at bbox", or "Identify page element type at bbox". Training with Cauldron was applied to avoid catastrophic forgetting.  -->

<!-- 
The model was trained on:
- 64 NVIDIA A100 80GB GPUs, 
- one epoch lasting 38 hours, 4 epochs in total.
- optimizer: AdamW
- learning rates: 2x 10^-4, 2x10^-6
- gradient clipping: 1.0
- warmup ratio 0.03


Achieved inference efficiency: 
- page conversion time: 0.35 seconds
- memory usage: 0.489GB VRAM
- max sequence length: 8192 tokens
- the model cam process 3 pages at a time

SmolDocling is a small but efficient vision-language model for document conversion.
It produces rich structured output in a single pass, which reduces error accumulation compared to multi-stage systems.
The model can link captions to images, preserve code formatting, and remove redundant headers or footers.
Typical issues include missing tags, malformed structure, and repetitive token loops.
Future work should improve page element localization for better accuracy.
Overall, SmolDocling shows that compact models with optimized formats can rival much larger models in multi-task document understanding. -->