<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Posts on Michal’s Log</title>
    <link>http://localhost:1313/posts/</link>
    <description>Recent content in Posts on Michal’s Log</description>
    <generator>Hugo -- 0.148.2</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 28 Nov 2025 16:42:36 +0200</lastBuildDate>
    <atom:link href="http://localhost:1313/posts/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Exploring Tiny Time Mixers Model</title>
      <link>http://localhost:1313/posts/exploring-tiny-time-mixers-model/</link>
      <pubDate>Fri, 28 Nov 2025 16:42:36 +0200</pubDate>
      <guid>http://localhost:1313/posts/exploring-tiny-time-mixers-model/</guid>
      <description>&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;2024 was a year where time series zero/few shot forecasting functioality made its way into LLM architectures: Moment, TimesFM, Chonos, Moirai, and many more showed up. Those were novel but, computationally, heavy to use. This triggered a second wave of lighter models incluging Tiny Time Mixers (TTM) architecture from IBM Research. TTM can work with just 1M parameters, supports  channel correlations and exogenous signals, and handles multivariate forecasting. Let&amp;rsquo;s dive in.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Granite Vision model notes</title>
      <link>http://localhost:1313/posts/granite-vision-model-notes/</link>
      <pubDate>Tue, 26 Aug 2025 16:42:36 +0200</pubDate>
      <guid>http://localhost:1313/posts/granite-vision-model-notes/</guid>
      <description>&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Many VLMs perform great on benchmarks like viusal question-answering or multi-image reasoning. Models in 2024 were predominantly trained on natural images, thus often limiting performance in other domains like visual document undestanding.&lt;/p&gt;
&lt;p&gt;Granite Vision is a 3 billion parameters model focused on entreprise use cases with visual document understanding document content extraction and working with documents. Get the Granite Vision research paper &lt;a href=&#34;https://arxiv.org/abs/2502.09927&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;architecture&#34;&gt;Architecture&lt;/h2&gt;
&lt;p&gt;Granite Vision was trained on 12 trillion tokens. Dataset for it&amp;rsquo;s training is constanlty curated by Granite Team but not open sourced. It contained 13 million images and 80 milllion instructions such as: document question-answering, scene understanding key-value extraction, text grounding, layout parsing, captioning, UI understanding, code comprehension.&lt;/p&gt;</description>
    </item>
    <item>
      <title>SmolDocling model notes</title>
      <link>http://localhost:1313/posts/smoldocling-model-notes/</link>
      <pubDate>Fri, 08 Aug 2025 16:42:36 +0200</pubDate>
      <guid>http://localhost:1313/posts/smoldocling-model-notes/</guid>
      <description>&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;OCR on documents remains a challenging task. While printed text can often be recognized with 95% or higher accuracy, real-world documents — containing handwriting, non-standard layouts, and other irregularities — are still much harder to read accurately. There are high quality systems solving documents reading in sub-tasks: OCR, layout analysis, structure recognition, classification.&lt;br&gt;
Recent trend is using large vision-language models to solve whole convertion task in one shot, and giving the user opportunity to define additional specific tasks to do in the prompt.
This post is about SmalDocling, a very tiny and compute-effient vision-language model doing the convertion task and the instruction task written into a prompt. The post is based on &lt;code&gt;SmolDocling&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2503.11576&#34;&gt;research paper&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
