<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Posts on Michal’s Log</title>
    <link>https://mlog.space/posts/</link>
    <description>Recent content in Posts on Michal’s Log</description>
    <generator>Hugo -- 0.148.2</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 26 Aug 2025 16:42:36 +0200</lastBuildDate>
    <atom:link href="https://mlog.space/posts/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Granite Vision model notes</title>
      <link>https://mlog.space/posts/granite-vision-model-notes/</link>
      <pubDate>Tue, 26 Aug 2025 16:42:36 +0200</pubDate>
      <guid>https://mlog.space/posts/granite-vision-model-notes/</guid>
      <description>&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Many VLMs perform great on benchmarks like viusal question-answering or multi-image reasoning. Models in 2024 were predominantly trained on natural images, thus often limiting performance in other domains like visual document undestanding.&lt;/p&gt;
&lt;p&gt;Granite Vision is a 3 billion parameters model focused on entreprise use cases with visual document understanding document content extraction and working with documents. Get the Granite Vision research paper &lt;a href=&#34;https://arxiv.org/abs/2502.09927&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;architecture&#34;&gt;Architecture&lt;/h2&gt;
&lt;p&gt;Granite Vision was trained on 12 trillion tokens. Dataset for it&amp;rsquo;s training is constanlty curated by Granite Team but not open sourced. It contained 13 million images and 80 milllion instructions such as: document question-answering, scene understanding key-value extraction, text grounding, layout parsing, captioning, UI understanding, code comprehension.&lt;/p&gt;</description>
    </item>
    <item>
      <title>SmolDocling model notes</title>
      <link>https://mlog.space/posts/smoldocling-model-notes/</link>
      <pubDate>Fri, 08 Aug 2025 16:42:36 +0200</pubDate>
      <guid>https://mlog.space/posts/smoldocling-model-notes/</guid>
      <description>&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;OCR on documents remains a challenging task. While printed text can often be recognized with 95% or higher accuracy, real-world documents — containing handwriting, non-standard layouts, and other irregularities — are still much harder to read accurately. There are high quality systems solving documents reading in sub-tasks: OCR, layout analysis, structure recognition, classification.&lt;br&gt;
Recent trend is using large vision-language models to solve whole convertion task in one shot, and giving the user opportunity to define additional specific tasks to do in the prompt.
This post is about SmalDocling, a very tiny and compute-effient vision-language model doing the convertion task and the instruction task written into a prompt. The post is based on &lt;code&gt;SmolDocling&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2503.11576&#34;&gt;research paper&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
